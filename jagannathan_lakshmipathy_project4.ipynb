{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_WpiPl1Txxh"
   },
   "source": [
    "# Project 4: Poisonous Mushrooms\n",
    "\n",
    "**Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w34qHYsPTxxl"
   },
   "source": [
    "In this project, you'll investigate properties of mushrooms. This classic dataset contains over 8000 examples, where each describes a mushroom by a variety of features like color, odor, etc., and the target variable is an indicator for whether the mushroom is poisonous. The feature space has been binarized. Look at the feature_names below to see all 126 binary names.\n",
    "\n",
    "You'll start by running PCA to reduce the dimensionality from 126 down to 2 so that you can easily visualize the data. In general, PCA is very useful for visualization (though sklearn.manifold.tsne is known to produce better visualizations). Recall that PCA is a linear transformation. The 1st projected dimension is the linear combination of all 126 original features that captures as much of the variance in the data as possible. The 2nd projected dimension is the linear combination of all 126 original features that captures as much of the remaining variance as possible. The idea of dense low dimensional representations is crucial to machine learning!\n",
    "\n",
    "Once you've projected the data to 2 dimensions, you'll experiment with clustering using k-means and density estimation with Gaussian mixture models (GMM). Finally, you'll train a classifier by fitting a GMM for the positive class and a GMM for the negative class, and perform inference by comparing the probabilities output by each model.\n",
    "\n",
    "As always, you're welcome to work on the project in groups and discuss ideas on the course wall, but please **prepare your own write-up and write your own code**.\n",
    "\n",
    "## Grading\n",
    "---\n",
    "- Make sure to answer every part in every question.\n",
    " - There are 6 equally weighted questions.\n",
    " - Read carefully what is asked including the notes.\n",
    " - Additional points may be deducted if:\n",
    "   - the code is not clean and well commented, \n",
    "   - and if the functions or answers are too long.\n",
    "\n",
    " ## Requirements:\n",
    "---\n",
    "1. Comment your code.\n",
    "1. All graphs should have titles, label for each axis, and if needed a legend. It should be understandable on its own.\n",
    "1. All code must run on colab.research.google.com\n",
    "1. You should not import any additional libraries.\n",
    "1. Try and minimize the use of the global namespace (meaning keep things in functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YiA2FUYrTxxl"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import urllib.request as urllib2 # For python3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rNHQPzslTxxo"
   },
   "outputs": [],
   "source": [
    "MUSHROOM_DATA = 'https://raw.githubusercontent.com/UCB-MIDS/207-Applied-Machine-Learning/master/Data/mushroom.data'\n",
    "MUSHROOM_MAP = 'https://raw.githubusercontent.com/UCB-MIDS/207-Applied-Machine-Learning/master/Data/mushroom.map'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSKrlpfkTxxp"
   },
   "source": [
    "Load feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "43pbKrDATxxr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded feature names:  126\n",
      "['cap-shape=bell', 'cap-shape=conical', 'cap-shape=convex', 'cap-shape=flat', 'cap-shape=knobbed', 'cap-shape=sunken', 'cap-surface=fibrous', 'cap-surface=grooves', 'cap-surface=scaly', 'cap-surface=smooth', 'cap-color=brown', 'cap-color=buff', 'cap-color=cinnamon', 'cap-color=gray', 'cap-color=green', 'cap-color=pink', 'cap-color=purple', 'cap-color=red', 'cap-color=white', 'cap-color=yellow', 'bruises?=bruises', 'bruises?=no', 'odor=almond', 'odor=anise', 'odor=creosote', 'odor=fishy', 'odor=foul', 'odor=musty', 'odor=none', 'odor=pungent', 'odor=spicy', 'gill-attachment=attached', 'gill-attachment=descending', 'gill-attachment=free', 'gill-attachment=notched', 'gill-spacing=close', 'gill-spacing=crowded', 'gill-spacing=distant', 'gill-size=broad', 'gill-size=narrow', 'gill-color=black', 'gill-color=brown', 'gill-color=buff', 'gill-color=chocolate', 'gill-color=gray', 'gill-color=green', 'gill-color=orange', 'gill-color=pink', 'gill-color=purple', 'gill-color=red', 'gill-color=white', 'gill-color=yellow', 'stalk-shape=enlarging', 'stalk-shape=tapering', 'stalk-root=bulbous', 'stalk-root=club', 'stalk-root=cup', 'stalk-root=equal', 'stalk-root=rhizomorphs', 'stalk-root=rooted', 'stalk-root=missing', 'stalk-surface-above-ring=fibrous', 'stalk-surface-above-ring=scaly', 'stalk-surface-above-ring=silky', 'stalk-surface-above-ring=smooth', 'stalk-surface-below-ring=fibrous', 'stalk-surface-below-ring=scaly', 'stalk-surface-below-ring=silky', 'stalk-surface-below-ring=smooth', 'stalk-color-above-ring=brown', 'stalk-color-above-ring=buff', 'stalk-color-above-ring=cinnamon', 'stalk-color-above-ring=gray', 'stalk-color-above-ring=orange', 'stalk-color-above-ring=pink', 'stalk-color-above-ring=red', 'stalk-color-above-ring=white', 'stalk-color-above-ring=yellow', 'stalk-color-below-ring=brown', 'stalk-color-below-ring=buff', 'stalk-color-below-ring=cinnamon', 'stalk-color-below-ring=gray', 'stalk-color-below-ring=orange', 'stalk-color-below-ring=pink', 'stalk-color-below-ring=red', 'stalk-color-below-ring=white', 'stalk-color-below-ring=yellow', 'veil-type=partial', 'veil-type=universal', 'veil-color=brown', 'veil-color=orange', 'veil-color=white', 'veil-color=yellow', 'ring-number=none', 'ring-number=one', 'ring-number=two', 'ring-type=cobwebby', 'ring-type=evanescent', 'ring-type=flaring', 'ring-type=large', 'ring-type=none', 'ring-type=pendant', 'ring-type=sheathing', 'ring-type=zone', 'spore-print-color=black', 'spore-print-color=brown', 'spore-print-color=buff', 'spore-print-color=chocolate', 'spore-print-color=green', 'spore-print-color=orange', 'spore-print-color=purple', 'spore-print-color=white', 'spore-print-color=yellow', 'population=abundant', 'population=clustered', 'population=numerous', 'population=scattered', 'population=several', 'population=solitary', 'habitat=grasses', 'habitat=leaves', 'habitat=meadows', 'habitat=paths', 'habitat=urban', 'habitat=waste', 'habitat=woods']\n"
     ]
    }
   ],
   "source": [
    "feature_names = []\n",
    "\n",
    "for line in urllib2.urlopen(MUSHROOM_MAP):\n",
    "    [index, name, junk] = line.decode('utf-8').split()\n",
    "    feature_names.append(name)\n",
    "\n",
    "print('Loaded feature names: ', len(feature_names))\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYgsKjyeTxxt"
   },
   "source": [
    "Load data. The dataset is sparse, but there aren't too many features, so we'll use a dense representation, which is supported by all sklearn objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEDKnm1sTxxt"
   },
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "\n",
    "for line in urllib2.urlopen(MUSHROOM_DATA):\n",
    "    items = line.decode('utf-8').split()\n",
    "    Y.append(int(items.pop(0)))\n",
    "    x = np.zeros(len(feature_names))\n",
    "    for item in items:\n",
    "        feature = int(str(item).split(':')[0])\n",
    "        x[feature] = 1\n",
    "    X.append(x)\n",
    "\n",
    "# Convert these lists to numpy arrays.\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Split into train and test data.\n",
    "train_data, train_labels = X[:7000], Y[:7000]\n",
    "test_data, test_labels = X[7000:], Y[7000:]\n",
    "\n",
    "# Check that the shapes look right.\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vXR9mODTxxu"
   },
   "source": [
    "### Question 1:PCA and fraction of total variance\n",
    "---\n",
    "\n",
    "1. Do a principal components analysis on the data.\n",
    "1. Print what fraction of the total variance in the training data is explained by the first k principal components, for k in [1, 2, 3, 4, 5, 10, 20, 30, 40, 50].\n",
    "1. Show a lineplot of fraction of total variance vs. number of principal components, for all possible numbers of principal components.\n",
    "1. You only need to call PCA.fit once.\n",
    "\n",
    "Notes:\n",
    "* The pandas DataFrame [cumsum](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.cumsum.html) function may be helpful.\n",
    "* You can use `PCA` to produce a PCA analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Verdana; color: green;\">ANSWER: <br> In the following snippet we performed the principal component analysis (PCA) for the provided training data. </span>\n",
    "\n",
    "<div style=\"font-family:Verdana; color: green;\">\n",
    "    <ol>\n",
    "        <li> we have used the built-in PCA component inside the sklearn.decomposition module to do the PCA. However, to understand the principal component analysis (PCA). First, we use the numpy cov() function to compute the covariance matrix for the training data. We then compute eigen values and vectors using numpy's eigh(). We then sort the eigen vectors by their respective eignen values. We then arrange the eigen pairs where the first value of the pair is the normalized eigen value and the second value of the pair is the eigen vector.</li>\n",
    "        <li> We have printed the fraction of total variance for k in [1, 2, 3, 4, 5, 10, 20, 30, 40, 50].</li>\n",
    "        <li> We have ploted the fraction of total variances vs. number of principal components.</li>\n",
    "        <li> Finally, as instructed we have used PCA.fit() only once.</li>\n",
    "    </ol>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYpzUrfxcqVR"
   },
   "outputs": [],
   "source": [
    "def P1():\n",
    "    ## STUDENT START ###\n",
    "    #cov_mat = np.cov(train_data.T)\n",
    "    #eig_vals, eig_vecs = np.linalg.eigh(cov_mat)\n",
    "    #tot = sum(eig_vals)\n",
    "    #eig_pairs = [(vl/tot, vc) for vl, vc in sorted(zip(eig_vals, eig_vecs), key=lambda x: x[0], reverse=True)]\n",
    "    #cum_eig_vals = np.cumsum(np.array([*eig_pairs])[:,0])\n",
    "    \n",
    "    x = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50]\n",
    "    #y = cum_eig_vals[x]\n",
    "    #plt.plot(x, y)\n",
    "        \n",
    "    pca = PCA(n_components=51)\n",
    "    pca.fit(train_data)\n",
    "    cum_var = np.cumsum(pca.explained_variance_/sum(pca.explained_variance_))\n",
    "    print(\"Fraction of total variance: \", cum_var[x])\n",
    "    #print(\"\", cum_eig_vals[x])\n",
    "    \n",
    "    plt.plot(x, cum_var[x])\n",
    "    ## STUDENT END ###\n",
    "\n",
    "P1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Verdana; color: green;\"> To understand the principal component analysis (PCA) we built the PCA from the scratch as follows. First, we use the numpy cov() function to compute the covariance matrix for the training data. We then compute eigen values and vectors using numpy's eigh(). We then sort the eigen vectors by their respective eignen values. We then arrange the eigen pairs where the first value of the pair is the normalized eigen value and the second value of the pair is the eigen vector. We have ploted the fraction of total variances against number of principal components as above to compare it with the plot above.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P1_from_scratch():\n",
    "    ## STUDENT START ###\n",
    "    # get the covariance matrix\n",
    "    cov_mat = np.cov(train_data.T)\n",
    "    \n",
    "    # calcualte the eigen values and vectors\n",
    "    eig_vals, eig_vecs = np.linalg.eigh(cov_mat)\n",
    "    \n",
    "    # compute the sum of all the eigen values\n",
    "    tot = sum(eig_vals)\n",
    "    \n",
    "    # compute eigen pairs normalized eignen value and its eigen vector and sort it by the value\n",
    "    eig_pairs = [(vl/tot, vc) for vl, vc in sorted(zip(eig_vals, eig_vecs), key=lambda x: x[0], reverse=True)]\n",
    "    cum_eig_vals = np.cumsum(np.array([*eig_pairs], dtype=object)[:,0])\n",
    "    \n",
    "    # plotted the cumulative eigen values and the count it makes it\n",
    "    x = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50]\n",
    "    y = cum_eig_vals[x]\n",
    "    plt.plot(x, y)\n",
    "    \n",
    "P1_from_scratch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1daqkzNTxxx"
   },
   "source": [
    "### Question 2: PCA for visualization\n",
    "\n",
    "PCA can be very useful for visualizing data. \n",
    "1. Project the training data down to 2 dimensions and show as a square scatterplot.\n",
    "  - Show poisonous examples (labeled 1) in red and non-poisonous examples in green (labeled 0)\n",
    "  - Here's a reference for plotting: http://matplotlib.org/users/pyplot_tutorial.html\n",
    "\n",
    "Notes:\n",
    "* You can use `PCA` to produce a PCA analysis.\n",
    "* Be sure to add a title, axis labels and a legend to your graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Verdana; color: green;\">ANSWER: <br> In the following snippet we performed the principal component analysis (PCA) for the provided training data. As we are interested in ploting the data in 2 dimensions, we chose the components to be 2 and transformed the trainng data: </span>\n",
    "\n",
    "<div style=\"font-family:Verdana; color: green;\">\n",
    "    <ol>\n",
    "        <li> We have color coded the examples with poisonous (label 1) in red and non-poisonous (label 0) in green.</li>\n",
    "        <li> we have plotted this using a scatter plot.</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXyB2s2oTxxy"
   },
   "outputs": [],
   "source": [
    "def Q2():\n",
    "    ### STUDENT START ###\n",
    "    # we are reducing the dmentions to 2 using PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X = pca.fit_transform(train_data)\n",
    "    \n",
    "    # plotting the poisonous and nonpoisonous in two dimensions with color red and green resp.\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.scatter(X[train_labels == 0][:, 0], X[train_labels == 0][:, 1], marker='+', c='green')\n",
    "    plt.scatter(X[train_labels == 1][:, 0], X[train_labels == 1][:, 1], marker='x', c='red')\n",
    "    \n",
    "    # limiting the axes for clarity\n",
    "    plt.ylim([-2.1, 3.1])\n",
    "    plt.xlim([-2.1, 3.1])\n",
    "    \n",
    "    plt.legend(['Non-poisonous','Poisonous'])\n",
    "    plt.title('Poisonous Mushrooms')\n",
    "    plt.ylabel('Component 1')\n",
    "    plt.xlabel('Component 2')\n",
    "    ### STUDENT END ###\n",
    "\n",
    "Q2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGHMr8wiTxxz"
   },
   "source": [
    "### Question 3: Visualizing GMMs\n",
    "---\n",
    "\n",
    "1. Fit a k-means cluster model with 6 clusters over the 2d projected data. \n",
    "  - As in part 2, show as a square scatterplot with the positive (poisonous) examples in red and the negative (non-poisonous) examples in green.  \n",
    "  - For each cluster, mark the centroid and plot a circle that goes through the cluster's example that is most distant from the centroid.\n",
    "\n",
    "Notes:\n",
    "* You can use `KMeans` to produce a k-means cluster analysis.\n",
    "* You can use `linalg.norm` to determine distance (dissimilarity) between observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Verdana; color: green;\">ANSWER: <br> In the following snippet we performed the k-means clustering of the projected 2d data for the provided training data. As in the previous problem we have created a color code scattered plot of both poisonous and non-poisonous mushrooms. </span>\n",
    "\n",
    "<div style=\"font-family:Verdana; color: green;\">\n",
    "    <ol>\n",
    "        <li> We have color coded the poisonous mushrooms in red and non-poisonous mushrooms in green.</li>\n",
    "        <li> Using the k-means clustering we clustered the data points into 6 clusters and have plotted the centroids and their corresponding circle with the most distance from the centroid.</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q3():\n",
    "    \n",
    "    ### STUDENT START ###\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # we are reducing the dmentions to 2 using PCA\n",
    "    model = PCA(n_components=2)\n",
    "    results = model.fit_transform(train_data)\n",
    "    \n",
    "    clusters = 6\n",
    "    plt.figure(figsize = (5,5))\n",
    "    \n",
    "    # plotting the poisonous and nonpoisonous in two dimensions with color red and green resp.\n",
    "    plt.plot(results[train_labels==1][:,0], results[train_labels==1][:,1], 'x', c='red')\n",
    "    plt.plot(results[train_labels==0][:,0], results[train_labels==0][:,1], '+', c='green')\n",
    "    \n",
    "    # limiting the axes for clarity\n",
    "    plt.ylim([-2.1, 3.1])\n",
    "    plt.xlim([-2.1, 3.1])\n",
    "\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.title(\"KMeans clusters=%d\" % clusters)\n",
    "    plt.legend([\"Poisonous\", \"Non-poisonous\"]) \n",
    "\n",
    "    # clustering the train data (dimension reduced train data)    \n",
    "    kmodel = KMeans(n_clusters=clusters).fit(results)\n",
    "    \n",
    "    # extracted the centers\n",
    "    kcenters = kmodel.cluster_centers_\n",
    "    kpredictions = kmodel.predict(results)\n",
    "    plt.scatter(kcenters[:, 0], kcenters[:, 1], c='black', s=30, zorder=3)\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    # for each cluster calculate the radii\n",
    "    for ix in range(clusters):\n",
    "        # Calculating radius\n",
    "        radii = max([np.linalg.norm(np.subtract(i,kcenters[ix])) for i in zip(results[kpredictions == ix, 0],results[kpredictions == ix, 1])])\n",
    "        # adding the circle to the plot\n",
    "        ax.add_patch(plt.Circle(kcenters[ix],radii,fill=False,alpha=0.5))\n",
    "    ### STUDENT END ###\n",
    "\n",
    "Q3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_O5V29yTxx0"
   },
   "source": [
    "### Question 4: Understanding GMMs with Density Plots\n",
    "---\n",
    "\n",
    "1. Fit Gaussian mixture models for the positive (poisonous) examples in your 2d projected data. \n",
    "  - Vary the number of mixture components from 1 to 4 and the covariance matrix type 'spherical', 'diag', 'tied', 'full' (that's 16 models).  \n",
    "  - Show a 4x4 grid of square-shaped plots of the estimated density contours. \n",
    "    - Each row showing a different number of mixture components. \n",
    "    - Each column showing a different convariance matrix type.  \n",
    "1. Be sure to add a title for each plot in the grid to indicate what parameters were used.\n",
    "1. How are the covariance types 'tied' and 'full' different? How do you see it in the plots?\n",
    "\n",
    "Notes:\n",
    "* You can use `GaussianMixture(n_components=..., covariance_type=..., random_state=12345)` to produce a Gaussian mixture model.\n",
    "* You can use `contour` in combination with other methods to plot contours, like in this example: http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html#example-mixture-plot-gmm-pdf-py\n",
    "* You can use `contour` without the `norm` and `levels` parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Verdana; color: green;\">ANSWER: <br> We created a PCA model with two components and transformed the data into two features. Then we picked the positive examples and did a gmm as follows:</span>\n",
    "\n",
    "<div style=\"font-family:Verdana; color: green;\">\n",
    "    <ol>\n",
    "        <li> Fit Gaussian mixture models for the positive examples from the 2d projected data. We varied the number of mixture components from 1 to 4 and the covariance matrix type to be one of {'spherical', 'diag', 'tied', 'full'}.\n",
    "             <ol type=\"a\">\n",
    "                <li>We have displayed a 4x4 grid of square-shaped plots of the estimated density contours.</li>\n",
    "                <li>Each row showing a different number of mixture components.</li>\n",
    "                <li>Each column showing a different convariance matrix type.</li>\n",
    "            </ol>\n",
    "        </li>\n",
    "        <li> We have added a title for each plot in the grid</li>\n",
    "        <li> From the scikit documentation, a \"tied\" covariance type means that for given set of components all contours will have the same shape and for different set of components the shape can be different. In contrast, a covariance type of \"full\" means that the contours independently adopt any position and shape even for a given set of components. As shown in column 3 of the plots below, we have showed the 'tied' covariance type. In those plots we notice that for components = 2  the shape of the contours look the same but we see different shapes between components = 2 and components = 3. For covariance type \"full\", we see different shaped contours even with in a given component size (e.g. components = 2).</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q4():\n",
    "    ### STUDENT START ###\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # we are reducing the dmentions to 2 using PCA\n",
    "    model = PCA(n_components=2)\n",
    "    results = model.fit_transform(train_data)\n",
    "    \n",
    "    # selecting only the positives\n",
    "    positives = results[train_labels == 1]\n",
    "    covar_types = ['spherical', 'diag', 'tied', 'full']\n",
    "    \n",
    "    fig, ax = plt.subplots(4, 4, figsize=(15, 15))\n",
    "    \n",
    "    # varying the components \n",
    "    for component in range(4):\n",
    "        # varying the types\n",
    "        for j, covar_type in enumerate(covar_types):\n",
    "            # fitting the GMM for the given components and type\n",
    "            gmm = GaussianMixture(n_components=component+1, covariance_type=covar_type).fit(positives)\n",
    "            \n",
    "            # create a uniform grid and calculate the scores at each sample point\n",
    "            x = np.linspace(-8.0, 8.0)\n",
    "            y = np.linspace(-8.0, 8.0)\n",
    "            X, Y = np.meshgrid(x, y)\n",
    "            XX = np.array([X.ravel(), Y.ravel()]).T\n",
    "            Z = -gmm.score_samples(XX)\n",
    "            Z = Z.reshape(X.shape)\n",
    "            \n",
    "            # plot the contours \n",
    "            CS = ax[component, j].contour(X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0), levels=np.logspace(0, 3, 10))\n",
    "            ax[component, j].scatter(positives[:, 0], positives[:, 1], .8)\n",
    "            ax[component, j].set_title(\"Comp = {}, Cov = {}\".format(component+1, covar_type))\n",
    "            \n",
    "  ### STUDENT END ###\n",
    "\n",
    "Q4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiaTfl2_Txx1"
   },
   "source": [
    "### Question 5: Using Unsupervised models for classification...?\n",
    "---\n",
    "\n",
    "1. Fit two Gaussian mixture models:\n",
    "  - On your 2d projected training data. \n",
    "    - Form a subset of all the poisonous examples and fit one GMM (gmm_poison).\n",
    "    - Form a subset of all the non-poisonous examples and fit another GMM (gmm_nonpoison).\n",
    "  - Use 4 mixture components and full convariance for each model.  \n",
    "1. Use the above trained GMMs to classify examples in your test set:\n",
    "   - For each example in the test set\n",
    "     - Measure how likely it is to have been \"generated\" by gmm_poison and gmm_nonpoison (using score_samples).\n",
    "     - Predict the more likely label.\n",
    "  - You can use score_samples.\n",
    "  - You can do this without a for loop which will be more efficient.\n",
    "1. What is the accuracy of the predictions on the test data? (we expect it to be about 0.95)\n",
    "\n",
    "Notes:\n",
    "* You can use `GaussianMixture(n_components=..., covariance_type=..., random_state=12345)` to produce a Gaussian mixture model.\n",
    "* You can use `GaussianMixture`'s `score_samples` method to find the probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Verdana; color: green;\">ANSWER: <br> We created a PCA model with two components and transformed the train and test data.</span>\n",
    "\n",
    "<div style=\"font-family:Verdana; color: green;\">\n",
    "    <ol>\n",
    "        <li> Fit Gaussian mixture models for the positive examples from the 2d projected data. We the mixture components of 4 and the covariance matrix type of 'full'.\n",
    "             <ol type=\"a\">\n",
    "                <li>We created two models gmm_poison, and gmm_nonpoison.</li>\n",
    "                <li>For each example in the test set we measured how likely it is to have been \"generated\" by gmm_poison and gmm_nonpoison and predicted the more likely label. We have used score_samples to get the logprobability.We did this without using a for loop as it will be efficient.</li>\n",
    "            </ol>\n",
    "        </li>\n",
    "        <li> We have shown the accuracy of the predictions (95.02%) on the test data.</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-QanSTeTxx_"
   },
   "outputs": [],
   "source": [
    "def Q5():\n",
    "    ### STUDENT START ###\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # reduce the dimensions to two\n",
    "    model = PCA(n_components=2)\n",
    "    train_results = model.fit_transform(train_data)\n",
    "    test_results = model.transform(test_data)\n",
    "    \n",
    "    # extract positives and negatives\n",
    "    train_positives = train_results[train_labels == 1]\n",
    "    train_negatives = train_results[train_labels == 0]\n",
    "    \n",
    "    # fit poison and nonpoison models\n",
    "    gmm_poison = GaussianMixture(n_components=4, covariance_type=\"full\").fit(train_positives)\n",
    "    gmm_nonpoison = GaussianMixture(n_components=4, covariance_type=\"full\").fit(train_negatives)\n",
    "    \n",
    "    # compute correct positives and correct negatives\n",
    "    correct_positives = np.sum(gmm_poison.score_samples(test_results[test_labels == 1])>\n",
    "                               gmm_nonpoison.score_samples(test_results[test_labels == 1]))\n",
    "    correct_negatives = np.sum(gmm_poison.score_samples(test_results[test_labels == 0])<\n",
    "                               gmm_nonpoison.score_samples(test_results[test_labels == 0]))\n",
    "    \n",
    "    # finally computing the accuracies\n",
    "    print('Accuracy of positives {}'.format(correct_positives/ len(test_labels[test_labels == 1])))\n",
    "    print('Accuracy of negatives {}'.format(correct_negatives/ len(test_labels[test_labels == 0])))\n",
    "    print('Total Accuracy {}'.format((correct_positives+correct_negatives)/ len(test_labels)))\n",
    "    ### STUDENT END ###\n",
    "\n",
    "Q5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf0v_azNTxyA"
   },
   "source": [
    "### Question 6: Understanding the GMM parameters\n",
    "---\n",
    "\n",
    "1. Run a series of experiments to find the Gaussian mixture model that results in the best accuracy with no more than 50 parameters.  Do this by varying the number of PCA components, the number of GMM components, and the covariance type.\n",
    "1. Print the best configuration and accuracy.\n",
    "1. Plot a scatter plot of accuracy vs #params.\n",
    "1. Bonus: Provide a better visualization by showing all four: accuracy, number of parameters, the covariance type and the number of PCA dimensions.\n",
    "\n",
    "\n",
    "\n",
    "Notes:\n",
    "* You can use `GaussianMixture(n_components=..., covariance_type=..., random_state=12345)` to produce a Gaussian mixture model.\n",
    "* [This spreadsheet](https://docs.google.com/spreadsheets/d/1LTEOU7q_Tgs4bX_3o2ePjDQrTRToZoc1J5XLlaRbmI0/edit?usp=sharing) should help (accessible from your google @berkeley.edu account)\n",
    "\n",
    "* Spherical - Each GMM component has a mean per dimension, and one variance. \n",
    "* Diag - Each GMM component has a mean per dimension and a variance for each dimension.\n",
    "* full - Each GMM component has a mean per dimension and a \"full\" covariance matrix. Only the covariance matrix is symmetric, so its $[i,j]$ entry would equal to its $[j,i]$ entry.\n",
    "* tied - Finally, tied has all components share a single covariance matrix. So each GMM component has a mean per dimension but they all share the same 'full' covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Verdana; color: green;\">ANSWER: <br> In this section we run experiments to find GMM that results in best accuracy with number of params no more than 50. We vary the PCA components and GMM components and the covariance types {'spherical', 'diag', 'tied', 'full'}.</span>\n",
    "\n",
    "<div style=\"font-family:Verdana; color: green;\">\n",
    "    <ol>\n",
    "        <li> We varied the PCA components between (1 and 12), GMM components between (1 and 50), covariance types from {'spherical', 'diag', 'tied', 'full'}. As we vary these we calculated the number of parameters. The formula for the number of components were provided the excel sheet provided. when the number of parameters exceed 50 we skip to the next step in the loop.  For the given number of PCA component, GMM component, and Covariance type we created a GMM using the train positive data (i.e. poisonous samples) and the train negative data (i.e. non-poisonous samples) and evaluated with the test positive and test negative data as shown in the snippet below. </li>\n",
    "        <li> As we go through the loops as outlined above, we measued the accuracy (as described above and shown in the snippet below) and record the accuracy against the PCA component, GMM component, and type. Though normally we refrain from using global variables here we have used to avoid passing these variables in and out of the functions and clutter the code. So, we have introduced 5 global variables to record pca components, gmm components, types, and parameter count. So after the execution of Q6() below, these global variables will be populated. We use these global variables in the plot functions namely plotAccuracies(), and plot3d().</li>\n",
    "        <li>The plotAccuracies() function plots the accuracies against paramcount.</li>\n",
    "        <li>The plot3d() function plots accuracy, number of parameters, the covariance type and the number of PCA components. </li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66XSs_95TxyA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pca_comps = []\n",
    "gmm_comps = []\n",
    "covar_types = []\n",
    "param_count = []\n",
    "accuracies = []\n",
    "\n",
    "def Q6():\n",
    "    ### STUDENT START ###\n",
    "    np.random.seed(0)\n",
    "    best_combination = ()\n",
    "    best_accuracy = 0.\n",
    "    \n",
    "    # varying PCA components\n",
    "    for pca_component in range(1, 12):\n",
    "        model = PCA(n_components=pca_component)\n",
    "        train_results = model.fit_transform(train_data)\n",
    "        test_results = model.transform(test_data)\n",
    "        \n",
    "        train_positives = train_results[train_labels == 1]\n",
    "        train_negatives = train_results[train_labels == 0]\n",
    "        \n",
    "        test_positives = test_results[test_labels == 1]\n",
    "        test_negatives = test_results[test_labels == 0]\n",
    "        \n",
    "        covar_map = {'spherical': 0, 'diag': 1, 'tied':2, 'full':3}\n",
    "        \n",
    "        # varying GMM components\n",
    "        for gmm_component in range(1, 50):\n",
    "            for covar_type in ['spherical', 'diag', 'tied', 'full']:\n",
    "                parameters_count = 0\n",
    "                \n",
    "                # computing the parameters for each covar type as defined in the excel\n",
    "                if covar_type == 'spherical':\n",
    "                    parameters_count = ((gmm_component-1) + pca_component * gmm_component + gmm_component) * 2\n",
    "                elif covar_type == 'diag':\n",
    "                    parameters_count = ((gmm_component-1) + pca_component * gmm_component + pca_component * gmm_component) * 2\n",
    "                elif covar_type == 'tied':\n",
    "                    parameters_count = ((gmm_component-1) + pca_component * gmm_component + pca_component * (pca_component + 1) / 2) * 2\n",
    "                elif covar_type == 'full':\n",
    "                    parameters_count = ((gmm_component-1) + pca_component * gmm_component + pca_component * (pca_component + 1) / 2 * gmm_component) * 2\n",
    "                \n",
    "                # skip the loop when the count is more than 50\n",
    "                if parameters_count > 50:\n",
    "                    continue\n",
    "                \n",
    "                # generate GMM model for positives and negatives\n",
    "                gmm_positives = GaussianMixture(n_components=gmm_component, covariance_type=covar_type, random_state=12345).fit(train_positives)\n",
    "                gmm_negatives = GaussianMixture(n_components=gmm_component, covariance_type=covar_type, random_state=12345).fit(train_negatives)\n",
    "                \n",
    "                # compute the correct positives and negatives\n",
    "                correct_positives = np.sum(gmm_positives.score_samples(test_positives)>gmm_negatives.score_samples(test_positives))\n",
    "                correct_negatives = np.sum(gmm_positives.score_samples(test_negatives)<gmm_negatives.score_samples(test_negatives))\n",
    "                correct_predictions = correct_positives + correct_negatives\n",
    "                \n",
    "                # update the best_accuracy \n",
    "                if best_accuracy < correct_predictions / len(test_labels):\n",
    "                    best_combination = (pca_component, gmm_component, covar_type, parameters_count, correct_predictions / len(test_labels))\n",
    "                    best_accuracy = correct_predictions / len(test_labels)\n",
    "                    \n",
    "                # record the components, types, counts and accuracies\n",
    "                pca_comps.append(pca_component)\n",
    "                gmm_comps.append(gmm_component)\n",
    "                covar_types.append(covar_map[covar_type])\n",
    "                param_count.append(parameters_count)\n",
    "                accuracies.append(correct_predictions / len(test_labels))\n",
    "                \n",
    "                # print the components, types, counts and accuracies\n",
    "                print (\"Processed: PCA = %d, GMM = (%d,%s), Parameters = %d, Accuracy = %f\" % (pca_component, gmm_component, covar_type, parameters_count, correct_predictions / len(test_labels)))\n",
    "                \n",
    "    # finally print the best components, types, counts and accuracies\n",
    "    print (\"\\nBest result: PCA = %d, GMM = (%d,%s), Parameters = %d, Accuracy = %f\" % best_combination)\n",
    "  ### STUDENT END ###\n",
    "\n",
    "Q6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the accuracies vs count\n",
    "def plotAccuracies(param_count, accuracies):\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.xlim([0, 53])\n",
    "    plt.scatter(np.array(param_count), np.array(accuracies), marker='+', c='green')\n",
    "\n",
    "# plot the comps, types and accuracies in 3D\n",
    "def plot3d(param_count, pca_comps, covar_types, accuracies):\n",
    "    x = np.array(pca_comps)\n",
    "    y = np.array(param_count)\n",
    "    c = np.array(covar_types)\n",
    "    z = np.array(accuracies)\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x[c == 0], y[c == 0], z[c==0], label='spherical')\n",
    "    ax.scatter(x[c == 1], y[c == 1], z[c==1], label='diag')\n",
    "    ax.scatter(x[c == 2], y[c == 2], z[c==2], label='tied')\n",
    "    ax.scatter(x[c == 3], y[c == 3], z[c==3], label='full')\n",
    "    ax.set_xlabel(\"pca components\")\n",
    "    ax.set_ylabel(\"# of parameters\")\n",
    "    ax.set_zlabel(\"accuracies\")\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "plotAccuracies(param_count, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3d(param_count, pca_comps, covar_types, accuracies)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "firstname_lastname_project4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
